---
layout: post
title: "GO-Video"
date: 2025-12-08
permalink: /2025/12/08/go-video.html
author: <b> Tz-Ying Wu, Sharath Nittur Sridhar, Subarna Tripathi </b>  
excerpt: "We propose GO-Tokenizer, which encodes grounded objects (GO) information at a more abstract level, into Video-LLMs referred to as GO-Video and is fine-tuned on the target dataset using low-rank adaptation ... "
permalink: /2025/12/08/go-video.html
---
<p>

</p>


<h3>
GO-Video Overview: <a href="https://arxiv.org/abs/2509.06335"> WACV 2026 </a> 
</h3>
<div class="text">
<p>
  We propose to improve the time-sensitive video understanding (TSV) capability of video large language models (Video-LLMs) with grounded objects (GO). 
  We hypothesize that TSV tasks can benefit from GO within frames, which is supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM for reasoning temporal localization. 
  While augmenting prompts with textual descriptions of these object annotations improves the performance of LITA, it also introduces extra token length and susceptibility to the noise in object-level information. 
  To address this, we propose GO-Tokenizer, a lightweight add-on module for Video-LLMs leveraging off-the-shelf object detectors to encode compact object information on the fly. 
  Experimental results demonstrate that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its counterpart, utilizing textual descriptions of objects in the prompt. 
  The gain generalizes across different models, datasets, and video understanding tasks, such as reasoning temporal localization and dense captioning.
</p>
