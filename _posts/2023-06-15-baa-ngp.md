---
layout: post
title: "Bundle-Adjusting Accelerated Neural Graphics Primitives"
date: 2023-06-15
permalink: /2023/06/15/baa-ngp.html
author: <b> Sainan Liu </b>
excerpt: "To address the current restrictions on knowing accurate camera poses a-priori as well as lengthy training time, we propose a novel approach called Bundle-Adjusting Accelerated Graphics Primitives (BAA-NGP) that can learn to estimate camera poses and optimize the radiance field simultaneously with 10 to 20 times speedup... "
permalink: /2023/06/15/baa-ngp.html
---
<p>
    <a href="https://arxiv.org/abs/2306.04166"> [Paper] </a>
    <a href="https://github.com/IntelLabs/baa-ngp"> [Code] </a>
</p>


<h2>
BAA-NGP Overview: 
</h2>
<div class="text">
<p>
Given a set of camera poses and associated images, Implicit neural representation (INR) models can be trained to synthesize novel, unseen views. Existing approaches like COLMAP and, most recently, bundle-adjusting neural radiance field methods often suffer from lengthy processing times. These delays ranging from hours to days, arise from laborious feature matching, hardware limitations, dense point sampling, and long training times required by a multi-layer perceptron structure with a large number of parameters. To address these challenges, we propose a framework called bundle-adjusting accelerated neural graphics primitives (BAA-NGP). Our approach leverages accelerated sampling and hash encoding to expedite both pose refinement/estimation and 3D scene reconstruction. Experimental results demonstrate that our method achieves a more than 10 to 20 times speed improvement in novel view synthesis compared to other bundle-adjusting neural radiance field methods without sacrificing the quality of pose estimation.
</p>





<h2>
NeRF(3D): Real scenes (LLFF dataset)
</h2>

<div markdown="0" id="carousel" class="carousel slide" data-ride="carousel" data-interval="4000" data-pause="hover" >
    <!-- Menu -->
    <ol class="carousel-indicators">
        <li data-target="#carousel" data-slide-to="0" class="active"></li>
        <li data-target="#carousel" data-slide-to="1"></li>
        <li data-target="#carousel" data-slide-to="2"></li>
        <li data-target="#carousel" data-slide-to="3"></li>
        <li data-target="#carousel" data-slide-to="4"></li>
    </ol>

    <!-- Items -->
    <div class="carousel-inner" markdown="0">
        <div class="item active">
            <img src="{{ site.url }}{{ site.baseurl }}/images/pubpic/baangp/flower_color.gif" alt="Animated GIF for flower." />
        </div>
        <div class="item">
            <img src="{{ site.url }}{{ site.baseurl }}/images/pubpic/baangp/horns_color.gif" alt="Animated GIF for horns." />
        </div>
        <div class="item">
            <img src="{{ site.url }}{{ site.baseurl }}/images/pubpic/baangp/leaves_color.gif" alt="Animated GIF for leaves." />
        </div>
        <div class="item">
            <img src="{{ site.url }}{{ site.baseurl }}/images/pubpic/baangp/trex_color.gif" alt="Animated GIF for trex." />
        </div>
        <div class="item">
            <img src="{{ site.url }}{{ site.baseurl }}/images/pubpic/baangp/fortress_color.gif" alt="Animated GIF for horfortress." />
        </div>        
    </div>
  <a class="left carousel-control" href="#carousel" role="button" data-slide="prev">
    <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>
    <span class="sr-only">Previous</span>
  </a>
  <a class="right carousel-control" href="#carousel" role="button" data-slide="next">
    <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span>
    <span class="sr-only">Next</span>
  </a>
</div>


<h2>
NeRF(3D): synthetic objects (Blender dataset)
</h2>

<table class="caption" style="width: 100%; margin: 0 auto -2pt auto; font-size: 80%;">
    <tbody>
        <tr>
            <td style="width: 24.5%">image</td>
            <td style="width: 24.5%">depth</td>
            <td style="width: 24.5%">image</td>
            <td style="width: 24.5%">depth</td>
        </tr>
    </tbody>
</table>

<style>
    img {
        max-width:100%;
        height: auto;
    }
</style>
<img src="/images/pubpic/baangp/blender.gif" alt="Animated GIF">

<table class="caption" style="width: 100%;">
    <tbody><tr>
        <td style="width: 49%">BARF</td>
        <td style="width: 49%">BAA-NGP</td>
    </tr>
    </tbody>
</table>

<p>
The figure shows test view synthesis. Similar to BARF, we start with imperfect camera pose estimation, and perform camera pose refinement and view synthesis simultaneously. Our method converges faster with clearer background and better details.
</p>

<h2>
Abstract
</h2>
<div class="text">
<p>
Implicit neural representation has emerged as a powerful method for reconstructing 3D scenes from 2D images. Given a set of camera poses and associated images, the models can be trained to synthesize novel, unseen views. In order to expand the use cases for implicit neural representations, we need to incorporate camera pose estimation capabilities as part of the representation learning, as this is necessary for reconstructing scenes from real-world video sequences where cameras are generally not being tracked. Existing approaches like COLMAP and, most recently, bundle-adjusting neural radiance field methods often suffer from lengthy processing times. These delays ranging from hours to days, arise from laborious feature matching, hardware limitations, dense point sampling, and long training times required by a multi-layer perceptron structure with a large number of parameters. To address these challenges, we propose a framework called bundle-adjusting accelerated neural graphics primitives (BAA-NGP). Our approach leverages accelerated sampling and hash encoding to expedite both pose refinement/estimation and 3D scene reconstruction. Experimental results demonstrate that our method achieves a more than 10 to 20 times speed improvement in novel view synthesis compared to other bundle-adjusting neural radiance field methods without sacrificing the quality of pose estimation.
</p>
</div>




            
